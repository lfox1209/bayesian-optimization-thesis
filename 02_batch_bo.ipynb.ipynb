{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6616308f",
   "metadata": {},
   "source": [
    "1. Files and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc189761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, glob, shutil, subprocess as sp\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06693391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Files that have to be adapted ===\n",
    "HOME = Path(\"/home/lfuchs\")\n",
    "SCRIPTS = HOME / \"masterarbeit\" / \"scripts\"\n",
    "CONFIG  = HOME / \"masterarbeit\" / \"config\"\n",
    "\n",
    "REQUEST_TPL_DIR = SCRIPTS / \"request_templates\"\n",
    "REQUEST_TPL_NAME = \"backtest_template.request.j2\" \n",
    "\n",
    "REQUEST_OUT_DIR  = HOME / \"simulation-requests\"\n",
    "RESULTS_BASE     = HOME / \"simulation-results\"\n",
    "\n",
    "GREPPER = Path(\"/home/mgalas/git/FWTClientRepo/src/main/python/fwt/tearsheets/Grepper.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66ea2f",
   "metadata": {},
   "source": [
    "2. Bayesian Optimisation Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3211cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function to load the search space --- \n",
    "# Note: the search space can be easily modified by changing the search_space.yaml template\n",
    "def load_search_space(path=CONFIG/\"search_space.yaml\"):\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17013ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function to create a request file ---\n",
    "# Note: the template only needs to be changed if DRACUS requires a new/updated request file format\n",
    "def generate_request_from_dict(params: dict, out_filename: str):\n",
    "    env = Environment(loader=FileSystemLoader(str(REQUEST_TPL_DIR)), autoescape=False, trim_blocks=True, lstrip_blocks=True)\n",
    "    tpl = env.get_template(REQUEST_TPL_NAME)\n",
    "    text = tpl.render(**params)\n",
    "\n",
    "    REQUEST_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = REQUEST_OUT_DIR / out_filename\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(text)\n",
    "    return str(out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc24d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function to generate the ZQQ.csv from the log.zip file ---\n",
    "def make_zqq_csv(result_dir: str | Path, subject_label: str) -> str:\n",
    "    result_dir = Path(result_dir)\n",
    "    deployed = result_dir / \"opt\" / \"simulations-service\" / \"deployed-jobs\" / subject_label\n",
    "\n",
    "    # 1) Find job-*-log.zip\n",
    "    zip_files = list(result_dir.glob(\"job-*-log.zip\"))\n",
    "    if not zip_files:\n",
    "        raise FileNotFoundError(f\"Kein job-*-log.zip in {deployed}\")\n",
    "    \n",
    "    # 2) Unzip → creates job-*-log.txt\n",
    "    sp.run(\n",
    "        [\"unzip\", \"-o\", str(zip_files[0])],\n",
    "        cwd=result_dir,\n",
    "        check=True,\n",
    "        stdout=sp.DEVNULL\n",
    "    )\n",
    "\n",
    "    # 3) Find job-*-log.txt\n",
    "    job_logs = sorted(deployed.glob(\"job-*-log.txt\"))\n",
    "    if not job_logs:\n",
    "        raise FileNotFoundError(f\"Nach unzip kein job-*-log.txt in {deployed} gefunden.\")\n",
    "    jobfile = str(job_logs[-1])\n",
    "\n",
    "    # 4) Run Grepper\n",
    "    cmd = [\"python3\", str(GREPPER), jobfile, subject_label]\n",
    "    sp.run(cmd, check=True)\n",
    "\n",
    "    zqq_src = deployed / f\"ZQQ_{subject_label}.csv\"\n",
    "    if not zqq_src.exists():\n",
    "        raise FileNotFoundError(f\"{zqq_src} wurde von Grepper.py nicht erzeugt.\")\n",
    "\n",
    "    # 5) Copy ZQQ into the main result directory\n",
    "    zqq_dst = result_dir / zqq_src.name\n",
    "    shutil.copy2(zqq_src, zqq_dst)\n",
    "    return str(zqq_dst)\n",
    "\n",
    "#Example usage:\n",
    "#make_zqq_csv(\"/home/lfuchs/simulation-results/bo_f1_t1\", \"bo_f1_t1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff4548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility function which calculates utility based on performance metrics in ZQQ.csv ---\n",
    "# Note: has to be updated if the utility function definition changes\n",
    "def utility_from_zqq(\n",
    "    zqq_csv: str | Path,\n",
    "    fold_start: str, fold_end: str,\n",
    "    crash_start: str,\n",
    "    return_components: bool = False,\n",
    "    lambdas=(0.35, 0.20, 0.15, 0.30)\n",
    "):\n",
    "   \n",
    "    lam1, lam2, lam3, lam4 = lambdas\n",
    "\n",
    "    df = pd.read_csv(zqq_csv)\n",
    "    date_col = df.columns[1]\n",
    "    fund_col = df.columns[2]\n",
    "    bench_col = df.columns[3]\n",
    "    df = df.rename(columns={date_col: \"Date\", fund_col: \"Fund\", bench_col: \"Benchmark\"})\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df = df.set_index(\"Date\").sort_index()\n",
    "\n",
    "    # Restrict to fold period\n",
    "    fold_start = pd.to_datetime(fold_start)\n",
    "    fold_end   = pd.to_datetime(fold_end)\n",
    "    df_fold = df.loc[(df.index >= fold_start) & (df.index <= fold_end)]\n",
    "    if df_fold.empty:\n",
    "        return float(\"-inf\")\n",
    "\n",
    "    # First available day t\n",
    "    t = df_fold.index[0]\n",
    "\n",
    "    # Helper: find next available date ≤ target\n",
    "    def pick_leq(target):\n",
    "        target = pd.to_datetime(target)\n",
    "        ix = df_fold.index.searchsorted(target, side=\"right\") - 1\n",
    "        if ix < 0: ix = 0\n",
    "        return df_fold.index[ix]\n",
    "\n",
    "    # Calculate relative returns\n",
    "    def rel(t1, t2):\n",
    "        a = df_fold.loc[t2, \"Fund\"] / df_fold.loc[t1, \"Fund\"]\n",
    "        m = df_fold.loc[t2, \"Benchmark\"] / df_fold.loc[t1, \"Benchmark\"]\n",
    "        return float(a), float(m)\n",
    "\n",
    "    # f1: 5y annualised minus 3.5%\n",
    "    t1 = pick_leq(t + pd.DateOffset(years=5))\n",
    "    a1, m1 = rel(t, t1)\n",
    "    f1 = (a1 ** (1/5)) - (m1 ** (1/5)) - 0.035\n",
    "\n",
    "    # f2: 1y relative\n",
    "    t2 = pick_leq(t + pd.DateOffset(years=1))\n",
    "    a2, m2 = rel(t, t2)\n",
    "    f2 = a2 - m2\n",
    "\n",
    "    # f3: 3m relative annualised (^4) + 0.2\n",
    "    t3 = pick_leq(t + pd.DateOffset(months=3))\n",
    "    a3, m3 = rel(t, t3)\n",
    "    f3 = (a3 ** 4) - (m3 ** 4) + 0.2\n",
    "\n",
    "    # f4: Integral over 2 years starting at crash_start\n",
    "    crash_start = pick_leq(pd.to_datetime(crash_start))\n",
    "    crash_end   = pick_leq(crash_start + pd.DateOffset(years=2))\n",
    "    df_win = df_fold.loc[(df_fold.index >= crash_start) & (df_fold.index <= crash_end)].copy()\n",
    "    if df_win.empty:\n",
    "        f4 = 0.0\n",
    "    else:\n",
    "        # Normalisation at crash start\n",
    "        Ac = float(df_fold.loc[crash_start, \"Fund\"])\n",
    "        Sc = float(df_fold.loc[crash_start, \"Benchmark\"])\n",
    "        fund_norm  = df_win[\"Fund\"] / Ac\n",
    "        bench_norm = df_win[\"Benchmark\"] / Sc\n",
    "        df_win[\"diff\"] = np.sqrt(fund_norm) - np.sqrt(bench_norm)\n",
    "        days = (df_win.index - df_win.index[0]).days.values\n",
    "        f4 = np.trapz(df_win[\"diff\"].values, x=days) / 2.0\n",
    "\n",
    "    U = lam1*f1 + lam2*f2 + lam3*f3 + lam4*f4\n",
    "\n",
    "    if return_components:\n",
    "        return float(U), float(f1), float(f2), float(f3), float(f4)\n",
    "    return float(U)\n",
    "\n",
    "#Example usage:\n",
    "#u, f1, f2, f3, f4 = utility_from_zqq(RESULTS_BASE / \"bo_f1_t0/\" \"ZQQ_bo_f1_t0.csv\", \"2005-01-03\", \"2009-12-31\", \"2007-07-19\", return_components=True)\n",
    "#print(f\"U={u:.4f}, f1={f1:.4f}, f2={f2:.4f}, f3={f3:.4f}, f4={f4:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f88e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function to sample hyperparameters from the defined search space ---\n",
    "# In our case: Generates NPORT, NFREQ and sector-specific Value/Growth weights for use in BO trials\n",
    "def sample_params_from_space(trial, space, start_date: str, end_date: str):\n",
    "    \n",
    "    nport = trial.suggest_int(\"NPORT\", space[\"nport\"][\"low\"], space[\"nport\"][\"high\"])\n",
    "    nfreq = trial.suggest_int(\"NFREQ\", space[\"nfreq\"][\"low\"], space[\"nfreq\"][\"high\"])\n",
    "\n",
    "    sectors = space[\"sectors\"]\n",
    "    low_w, high_w = space[\"weight_bounds\"][\"low\"], space[\"weight_bounds\"][\"high\"]\n",
    "\n",
    "    VE, GE = {}, {}\n",
    "    params = {\n",
    "        \"STARTDATE\": start_date,\n",
    "        \"ENDDATE\": end_date,\n",
    "        \"NPORT\": int(nport),\n",
    "        \"NFREQ\": int(nfreq),\n",
    "        \"VE\": VE,       \n",
    "        \"GE\": GE,\n",
    "    }\n",
    "\n",
    "    # Important: Add sequentially numbered keys to match {{ VE1 }}, {{ VE2 }} etc. and {{ GE1 }}, {{ GE2 }} etc. in the template\n",
    "    for i, sec in enumerate(sectors, start=1):\n",
    "        v = trial.suggest_float(f\"VE::{sec}\", low_w, high_w)\n",
    "        g = trial.suggest_float(f\"GE::{sec}\", low_w, high_w)\n",
    "        VE[sec] = v\n",
    "        GE[sec] = g\n",
    "        params[f\"VE{i}\"] = v   \n",
    "        params[f\"GE{i}\"] = g \n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3411cf5a",
   "metadata": {},
   "source": [
    "3. Sequential Bayesian Optimisation Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f805a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Definition of cross-validation folds ---\n",
    "# Note: folds must be adapted; crash dates are required for f4 (crash-resilience component of the utility function); \n",
    "# multiple evaluation periods are needed because the utility is always defined on 5-year windows\n",
    "FOLDS = {\n",
    "    \"f1\": {\n",
    "        \"req_start\":  \"2001-01-02\",\n",
    "        \"req_end\":    \"2009-12-31\",\n",
    "        \"eval_start\": \"2005-01-03\",\n",
    "        \"eval_end\":   \"2009-12-31\",\n",
    "        \"crash\":      \"2007-07-19\",\n",
    "    },\n",
    "    \"f2\": {\n",
    "        \"req_start\":  \"2006-01-03\",\n",
    "        \"req_end\":    \"2014-12-31\",\n",
    "        \"eval_start\": \"2010-01-04\",\n",
    "        \"eval_end\":   \"2014-12-31\",\n",
    "        \"crash\":      \"2011-07-21\",\n",
    "    },\n",
    "    \"f3\": {\n",
    "        \"req_start\":  \"2011-01-03\",\n",
    "        \"req_end\":    \"2019-12-31\",\n",
    "        \"eval_start\": \"2015-01-02\",\n",
    "        \"eval_end\":   \"2019-12-31\",\n",
    "        \"crash\":      \"2018-09-20\",\n",
    "    },\n",
    "    \"f4\": {\n",
    "        \"req_start\":  \"2016-01-04\",\n",
    "        \"req_end\":    \"2024-12-31\",\n",
    "        \"eval_start\": \"2020-01-02\",\n",
    "        \"eval_end\":   \"2024-12-31\",\n",
    "        \"crash\":      \"2020-03-23\",\n",
    "    },\n",
    "     \"f5\": {\n",
    "        \"req_start\": \"2001-01-02\",\n",
    "        \"req_end\":   \"2015-12-31\",\n",
    "        \"eval1_start\": \"2005-01-03\",\n",
    "        \"eval1_end\":   \"2009-12-31\",\n",
    "        \"crash1\":      \"2007-07-19\",\n",
    "        \"eval2_start\": \"2010-01-04\",\n",
    "        \"eval2_end\":   \"2014-12-31\",\n",
    "        \"crash2\":      \"2011-07-21\",\n",
    "    },\n",
    "\n",
    "    \"f6\": {\n",
    "        \"req_start\": \"2001-01-02\",\n",
    "        \"req_end\":   \"2020-12-31\",\n",
    "        \"eval1_start\": \"2005-01-03\",\n",
    "        \"eval1_end\":   \"2009-12-31\",\n",
    "        \"crash1\":      \"2007-07-19\",\n",
    "        \"eval2_start\": \"2010-01-04\",\n",
    "        \"eval2_end\":   \"2014-12-31\",\n",
    "        \"crash2\":      \"2011-07-21\",\n",
    "        \"eval3_start\": \"2015-01-02\",\n",
    "        \"eval3_end\":   \"2019-12-31\",\n",
    "        \"crash3\":      \"2018-09-20\",\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f90bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Objective function for BO runs on Fold 1 ---\n",
    "# Submits request, waits for DRACUS results, generates ZQQ.csv and computes utility\n",
    "def objective_real(trial,\n",
    "                   fold_name: str,\n",
    "                   req_start: str, req_end: str,      \n",
    "                   eval_start: str, eval_end: str,          \n",
    "                   crash_start: str,\n",
    "                   wait_hours=6, poll_secs=60):\n",
    "    \n",
    "    # Generate parameters from the defined search space\n",
    "    space = load_search_space()\n",
    "    params = sample_params_from_space(trial, space, req_start, req_end)\n",
    "\n",
    "    # Create unique label and write request file\n",
    "    label = f\"bo_{fold_name}_t{trial.number}\"\n",
    "    req_file = f\"{label}.request\"\n",
    "    generate_request_from_dict(params, req_file)\n",
    "\n",
    "    # Wait for result directory\n",
    "    result_dir = RESULTS_BASE / label\n",
    "    deadline = time.time() + wait_hours*3600\n",
    "    while time.time() < deadline:\n",
    "        if result_dir.exists():\n",
    "            break\n",
    "        time.sleep(poll_secs)\n",
    "    if not result_dir.exists():\n",
    "        return float(\"-inf\")\n",
    "\n",
    "    # Generate ZQQ.csv\n",
    "    zqq_path = make_zqq_csv(result_dir, label)\n",
    "\n",
    "    # Compute utility and store components\n",
    "    u, f1, f2, f3, f4 = utility_from_zqq(zqq_path, eval_start, eval_end, crash_start, return_components=True)\n",
    "    trial.set_user_attr(\"f1\", f1)\n",
    "    trial.set_user_attr(\"f2\", f2)\n",
    "    trial.set_user_attr(\"f3\", f3)\n",
    "    trial.set_user_attr(\"f4\", f4)\n",
    "    \n",
    "    return float(u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd498f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run BO study on fold f1 ---\n",
    "# Note: adjust n_trials as needed\n",
    "fold = FOLDS[\"f1\"]\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"real_f1\",\n",
    "                            storage=\"sqlite:///real_f1.db\", load_if_exists=True)\n",
    "\n",
    "study.optimize(\n",
    "    lambda t: objective_real(t, \"f1\",\n",
    "                             fold[\"req_start\"], fold[\"req_end\"],\n",
    "                             fold[\"eval_start\"], fold[\"eval_end\"],\n",
    "                             fold[\"crash\"]),\n",
    "    n_trials=4,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"Best value:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b76ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- Objective function for Fold 5 ---\n",
    "# Note: same as other folds, but utility is averaged over two 5-year evaluation periods (2005–2009 + 2010–2014)\n",
    "def objective_real_f5(trial,\n",
    "                      fold_name: str,\n",
    "                      req_start: str, req_end: str,\n",
    "                      eval1_start: str, eval1_end: str, crash1: str,\n",
    "                      eval2_start: str, eval2_end: str, crash2: str,\n",
    "                      wait_hours=6, poll_secs=60):\n",
    "\n",
    "    space = load_search_space()\n",
    "    params = sample_params_from_space(trial, space, req_start, req_end)\n",
    "\n",
    "    label = f\"bo_{fold_name}_t{trial.number}\"\n",
    "    req_file = f\"{label}.request\"\n",
    "    generate_request_from_dict(params, req_file)\n",
    "\n",
    "    result_dir = RESULTS_BASE / label\n",
    "    deadline = time.time() + wait_hours*3600\n",
    "    while time.time() < deadline:\n",
    "        if result_dir.exists():\n",
    "            break\n",
    "        time.sleep(poll_secs)\n",
    "    if not result_dir.exists():\n",
    "        return float(\"-inf\")\n",
    "\n",
    "    zqq_path = make_zqq_csv(result_dir, label)\n",
    "\n",
    "    u1, a1, b1, c1, d1 = utility_from_zqq(zqq_path, eval1_start, eval1_end, crash1, return_components=True)\n",
    "    u2, a2, b2, c2, d2 = utility_from_zqq(zqq_path, eval2_start, eval2_end, crash2, return_components=True)\n",
    "    u_avg = 0.5 * (u1 + u2)\n",
    "\n",
    "    trial.set_user_attr(\"f1\", 0.5 * (a1 + a2))\n",
    "    trial.set_user_attr(\"f2\", 0.5 * (b1 + b2))\n",
    "    trial.set_user_attr(\"f3\", 0.5 * (c1 + c2))\n",
    "    trial.set_user_attr(\"f4\", 0.5 * (d1 + d2))\n",
    "\n",
    "    return float(u_avg)\n",
    "\n",
    "\n",
    "# --- Run BO study on fold f5 ---\n",
    "# Note: adjust n_trials as needed\n",
    "fold5 = FOLDS[\"f5\"]\n",
    "\n",
    "study5 = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"real_f5\",\n",
    "    storage=\"sqlite:///real_f5.db\",\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "study5.optimize(\n",
    "    lambda t: objective_real_f5(\n",
    "        t, \"f5\",\n",
    "        req_start=fold5[\"req_start\"], req_end=fold5[\"req_end\"],\n",
    "        eval1_start=fold5[\"eval1_start\"], eval1_end=fold5[\"eval1_end\"], crash1=fold5[\"crash1\"],\n",
    "        eval2_start=fold5[\"eval2_start\"], eval2_end=fold5[\"eval2_end\"], crash2=fold5[\"crash2\"],\n",
    "    ),\n",
    "    n_trials=20,\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "\n",
    "print(\"REAL f5 best value:\", study5.best_value)\n",
    "print(\"REAL f5 best params:\", study5.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2516002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- Objective function for Fold 6 ---\n",
    "# Note: same as other folds, but utility is averaged over three 5-year evaluation periods (2005–2009 + 2010–2014 + 2015–2019) ---\n",
    "def objective_real_f6(trial,\n",
    "                      fold_name: str,\n",
    "                      req_start: str, req_end: str,\n",
    "                      eval1_start: str, eval1_end: str, crash1: str,\n",
    "                      eval2_start: str, eval2_end: str, crash2: str,\n",
    "                      eval3_start: str, eval3_end: str, crash3: str,\n",
    "                      wait_hours=6, poll_secs=60):\n",
    "\n",
    "    space = load_search_space()\n",
    "    params = sample_params_from_space(trial, space, req_start, req_end)\n",
    "\n",
    "    label = f\"bo_{fold_name}_t{trial.number}\"\n",
    "    req_file = f\"{label}.request\"\n",
    "    generate_request_from_dict(params, req_file)\n",
    "\n",
    "    result_dir = RESULTS_BASE / label\n",
    "    deadline = time.time() + wait_hours*3600\n",
    "    while time.time() < deadline:\n",
    "        if result_dir.exists():\n",
    "            break\n",
    "        time.sleep(poll_secs)\n",
    "    if not result_dir.exists():\n",
    "        return float(\"-inf\")\n",
    "\n",
    "    zqq_path = make_zqq_csv(result_dir, label)\n",
    "\n",
    "    #new\n",
    "    u1, a1, b1, c1, d1 = utility_from_zqq(zqq_path, eval1_start, eval1_end, crash1, return_components=True)\n",
    "    u2, a2, b2, c2, d2 = utility_from_zqq(zqq_path, eval2_start, eval2_end, crash2, return_components=True)\n",
    "    u3, a3, b3, c3, d3 = utility_from_zqq(zqq_path, eval3_start, eval3_end, crash3, return_components=True)\n",
    "    u_avg = (u1 + u2 + u3) / 3.0\n",
    "\n",
    "    trial.set_user_attr(\"f1\", (a1 + a2 + a3) / 3.0)\n",
    "    trial.set_user_attr(\"f2\", (b1 + b2 + b3) / 3.0)\n",
    "    trial.set_user_attr(\"f3\", (c1 + c2 + c3) / 3.0)\n",
    "    trial.set_user_attr(\"f4\", (d1 + d2 + d3) / 3.0)\n",
    "\n",
    "    return float(u_avg)\n",
    "\n",
    "\n",
    "# --- Run BO study on fold f6 ---\n",
    "# Note: adjust n_trials as needed\n",
    "fold6 = FOLDS[\"f6\"]\n",
    "\n",
    "study6 = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"real_f6\",\n",
    "    storage=\"sqlite:///real_f6.db\",\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "study6.optimize(\n",
    "    lambda t: objective_real_f6(\n",
    "        t, \"f6\",\n",
    "        req_start=fold6[\"req_start\"], req_end=fold6[\"req_end\"],\n",
    "        eval1_start=fold6[\"eval1_start\"], eval1_end=fold6[\"eval1_end\"], crash1=fold6[\"crash1\"],\n",
    "        eval2_start=fold6[\"eval2_start\"], eval2_end=fold6[\"eval2_end\"], crash2=fold6[\"crash2\"],\n",
    "        eval3_start=fold6[\"eval3_start\"], eval3_end=fold6[\"eval3_end\"], crash3=fold6[\"crash3\"],\n",
    "    ),\n",
    "    n_trials=20,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"REAL f6 best value:\", study6.best_value)\n",
    "print(\"REAL f6 best params:\", study6.best_trial.params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f425de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load completed BO studies ---\n",
    "#study = optuna.load_study(study_name=\"real_f1\", storage=\"sqlite:///real_f1.db\")\n",
    "#df1 = study.trials_dataframe(attrs=(\"params\", \"user_attrs\", \"value\", \"state\", \"datetime_start\", \"datetime_complete\"))\n",
    "#df1.head(25)\n",
    "\n",
    "#study5 = optuna.load_study(study_name=\"real_f5\", storage=\"sqlite:///real_f5.db\")\n",
    "#df5 = study5.trials_dataframe(attrs=(\"params\", \"user_attrs\", \"value\", \"state\", \"datetime_start\", \"datetime_complete\"))\n",
    "#df5.head(25)\n",
    "\n",
    "#study6 = optuna.load_study(study_name=\"real_f6\", storage=\"sqlite:///real_f6.db\")\n",
    "#df6 = study6.trials_dataframe(attrs=(\"params\", \"user_attrs\", \"value\", \"state\", \"datetime_start\", \"datetime_complete\"))\n",
    "#df6.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1ac1b7",
   "metadata": {},
   "source": [
    "4. Batch (Constant Liar) Bayesian Optimisation Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c8266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Send request file and return the label ---\n",
    "def submit_job_f1(trial, fold, label_prefix=\"cl_f1\"):\n",
    "    space = load_search_space()\n",
    "    params = sample_params_from_space(trial, space, fold[\"req_start\"], fold[\"req_end\"])\n",
    "\n",
    "    label = f\"{label_prefix}_t{trial.number}\"\n",
    "    req_file = f\"{label}.request\"\n",
    "    generate_request_from_dict(params, req_file)\n",
    "    return label\n",
    "\n",
    "\n",
    "# --- Make ZQQ.csv and calculate utility value ---\n",
    "def score_job_f1(label, fold, wait_hours=8, poll_secs=60):\n",
    "    result_dir = RESULTS_BASE / label\n",
    "\n",
    "    deadline = time.time() + wait_hours * 3600\n",
    "    while time.time() < deadline:\n",
    "        if result_dir.exists():\n",
    "            break\n",
    "        time.sleep(poll_secs)\n",
    "    if not result_dir.exists():\n",
    "        return float(\"-inf\"), None, None, None, None, None\n",
    "\n",
    "    zqq_path = make_zqq_csv(result_dir, label)\n",
    "\n",
    "    U, f1, f2, f3, f4 = utility_from_zqq(\n",
    "        zqq_path, fold[\"eval_start\"], fold[\"eval_end\"], fold[\"crash\"],\n",
    "        return_components=True\n",
    "    )\n",
    "    return float(U), float(f1), float(f2), float(f3), float(f4), str(zqq_path)\n",
    "\n",
    "\n",
    "# --- Batch BO run with Constant Liar on Fold f1 ---\n",
    "def cl_run_f1(\n",
    "    batch_size=3,\n",
    "    n_batches=3,\n",
    "    lie_policy=\"best\",          \n",
    "    storage=\"sqlite:///cl_f1.db\",\n",
    "    study_name=\"cl_f1\",\n",
    "    seed=42\n",
    "):\n",
    "    fold = FOLDS[\"f1\"]\n",
    "\n",
    "    # Real study (only stores true values)\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=study_name,\n",
    "                                storage=storage, load_if_exists=True,\n",
    "                                sampler=optuna.samplers.TPESampler(seed=seed))\n",
    "\n",
    "    # Shadow study (only for lies, in-memory)\n",
    "    shadow = optuna.create_study(direction=study.direction,\n",
    "                                 sampler=optuna.samplers.TPESampler(seed=seed))\n",
    "\n",
    "    # Helper function to determine liar value based on policy\n",
    "    def liar_value():\n",
    "        vals = [t.value for t in study.trials if t.value is not None]\n",
    "        if not vals:\n",
    "            return 0.0\n",
    "        if lie_policy == \"best\":\n",
    "            return max(vals)\n",
    "        if lie_policy == \"mean\":\n",
    "            return mean(vals)\n",
    "        if lie_policy == \"pessimistic\":\n",
    "            return min(vals)\n",
    "        return mean(vals)\n",
    "\n",
    "    for b in range(n_batches):\n",
    "        trials_real = []\n",
    "        labels = []\n",
    "\n",
    "        # Propose a batch via shadow             \n",
    "        for _ in range(batch_size):\n",
    "            t_real = study.ask()\n",
    "            t_shadow = shadow.ask()\n",
    "            \n",
    "            space = load_search_space()\n",
    "            _ = sample_params_from_space(t_real,   space, fold[\"req_start\"], fold[\"req_end\"])\n",
    "            _ = sample_params_from_space(t_shadow, space, fold[\"req_start\"], fold[\"req_end\"])\n",
    "            shadow.tell(t_shadow, liar_value()) # immediately lie to the shadow trial\n",
    "\n",
    "            # Submit request file\n",
    "            label = submit_job_f1(t_real, fold, label_prefix=f\"cl_f1_b{b}\")\n",
    "            trials_real.append(t_real)\n",
    "            labels.append(label)\n",
    "\n",
    "        # Score all batch jobs and record true values\n",
    "        for t_real, label in zip(trials_real, labels):\n",
    "\n",
    "            U, f1, f2, f3, f4, zqq_path = score_job_f1(label, fold, wait_hours=8, poll_secs=60)\n",
    "            t_real.set_user_attr(\"f1\", f1)\n",
    "            t_real.set_user_attr(\"f2\", f2)\n",
    "            t_real.set_user_attr(\"f3\", f3)\n",
    "            t_real.set_user_attr(\"f4\", f4)\n",
    "            t_real.set_user_attr(\"zqq_path\", zqq_path)\n",
    "\n",
    "            study.tell(t_real, float(U))\n",
    "            print(f\"[CL f1] trial {t_real.number} -> U={U:.4f} (f1={f1:.4f}, f2={f2:.4f}, f3={f3:.4f}, f4={f4:.4f})\")\n",
    "\n",
    "    print(\"CL f1 best value:\", study.best_value)\n",
    "    print(\"CL f1 best params:\", study.best_trial.params)\n",
    "    return study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153dedb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run CL study on fold f1 ---\n",
    "# Note: adjust batch_size and n_batches as needed\n",
    "study_cl_f1 = cl_run_f1(batch_size=3, n_batches=7, lie_policy=\"average\")\n",
    "\n",
    "# --- Load completed CL study ---\n",
    "study = optuna.load_study(study_name=\"cl_f1\", storage=\"sqlite:///cl_f1.db\")\n",
    "df_cl1 = study.trials_dataframe(attrs=(\"params\", \"user_attrs\", \"value\", \"state\", \"datetime_start\", \"datetime_complete\"))\n",
    "df_cl1.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79fc7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- Objective function for Fold 5 ---\n",
    "# Note: same as Fold 1, but utility is averaged over two 5-year evaluation periods (2005–2009 + 2010–2014)\n",
    "\n",
    "def submit_job_f5(trial, fold, label_prefix=\"cl_f5\"):\n",
    "    space = load_search_space()\n",
    "    params = sample_params_from_space(trial, space, fold[\"req_start\"], fold[\"req_end\"])\n",
    "    label = f\"{label_prefix}_t{trial.number}\"\n",
    "    req_file = f\"{label}.request\"\n",
    "    generate_request_from_dict(params, req_file)\n",
    "    return label\n",
    "\n",
    "def score_job_f5(label, fold, wait_hours=8, poll_secs=60):\n",
    "    result_dir = RESULTS_BASE / label\n",
    "\n",
    "    deadline = time.time() + wait_hours * 3600\n",
    "    while time.time() < deadline:\n",
    "        if result_dir.exists():\n",
    "            break\n",
    "        time.sleep(poll_secs)\n",
    "    if not result_dir.exists():\n",
    "        return float(\"-inf\"), None, None, None, None, None\n",
    "\n",
    "    zqq_path = make_zqq_csv(result_dir, label)\n",
    "\n",
    "    # Compute utility as average over two evaluation periods (different to Fold 1)\n",
    "    u1, f1a, f2a, f3a, f4a = utility_from_zqq(\n",
    "        zqq_path,\n",
    "        fold[\"eval1_start\"], fold[\"eval1_end\"], fold[\"crash1\"],\n",
    "        return_components=True\n",
    "    )\n",
    "    u2, f1b, f2b, f3b, f4b = utility_from_zqq(\n",
    "        zqq_path,\n",
    "        fold[\"eval2_start\"], fold[\"eval2_end\"], fold[\"crash2\"],\n",
    "        return_components=True\n",
    "    )\n",
    "\n",
    "    U  = (u1  + u2)  / 2.0\n",
    "    f1 = (f1a + f1b) / 2.0\n",
    "    f2 = (f2a + f2b) / 2.0\n",
    "    f3 = (f3a + f3b) / 2.0\n",
    "    f4 = (f4a + f4b) / 2.0\n",
    "\n",
    "    return float(U), float(f1), float(f2), float(f3), float(f4), str(zqq_path)\n",
    "\n",
    "\n",
    "def cl_run_f5(\n",
    "    batch_size=4,\n",
    "    n_batches=5,\n",
    "    lie_policy=\"mean\",         \n",
    "    storage=\"sqlite:///cl_f5.db\",\n",
    "    study_name=\"cl_f5\",\n",
    "    seed=42\n",
    "):\n",
    "    fold = FOLDS[\"f5\"] \n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=study_name,\n",
    "                                storage=storage, load_if_exists=True,\n",
    "                                sampler=optuna.samplers.TPESampler(seed=seed))\n",
    "\n",
    "    shadow = optuna.create_study(direction=study.direction,\n",
    "                                 sampler=optuna.samplers.TPESampler(seed=seed))\n",
    "\n",
    "    def liar_value():\n",
    "        vals = [t.value for t in study.trials if t.value is not None]\n",
    "        if not vals:\n",
    "            return 0.0\n",
    "        if lie_policy == \"best\":\n",
    "            return max(vals)\n",
    "        if lie_policy == \"mean\":\n",
    "            return mean(vals)\n",
    "        if lie_policy == \"pessimistic\":\n",
    "            return min(vals)\n",
    "        return mean(vals)\n",
    "\n",
    "    for b in range(n_batches):\n",
    "        trials_real = []     \n",
    "        labels = []          \n",
    "        for _ in range(batch_size):\n",
    "            t_real = study.ask()         \n",
    "            t_shadow = shadow.ask()\n",
    "            \n",
    "            space = load_search_space()\n",
    "            _ = sample_params_from_space(t_real,   space, fold[\"req_start\"], fold[\"req_end\"])\n",
    "            _ = sample_params_from_space(t_shadow, space, fold[\"req_start\"], fold[\"req_end\"])\n",
    "\n",
    "            shadow.tell(t_shadow, liar_value())\n",
    "\n",
    "            label = submit_job_f5(t_real, fold, label_prefix=f\"cl_f5_b{b}\")\n",
    "            trials_real.append(t_real)\n",
    "            labels.append(label)\n",
    "\n",
    "        for t_real, label in zip(trials_real, labels):\n",
    "\n",
    "            U, f1, f2, f3, f4, zqq_path = score_job_f5(label, fold, wait_hours=8, poll_secs=60)\n",
    "            t_real.set_user_attr(\"f1\", f1)\n",
    "            t_real.set_user_attr(\"f2\", f2)\n",
    "            t_real.set_user_attr(\"f3\", f3)\n",
    "            t_real.set_user_attr(\"f4\", f4)\n",
    "            t_real.set_user_attr(\"zqq_path\", zqq_path)\n",
    "\n",
    "            study.tell(t_real, float(U))\n",
    "            print(f\"[CL f5] trial {t_real.number} -> U={U:.4f} (f1={f1:.4f}, f2={f2:.4f}, f3={f3:.4f}, f4={f4:.4f})\")\n",
    "\n",
    "    print(\"CL f5 best value:\", study.best_value)\n",
    "    print(\"CL f5 best params:\", study.best_trial.params)\n",
    "    return study  \n",
    "\n",
    "\n",
    "# --- Run CL study on fold f5 ---\n",
    "# Note: adjust batch_size and n_batches as needed\n",
    "study_cl_f5 = cl_run_f5(batch_size=4, n_batches=1, lie_policy=\"mean\")\n",
    "\n",
    "\n",
    "# --- Load completed CL study ---\n",
    "study_cl5 = optuna.load_study(study_name=\"cl_f5\", storage=\"sqlite:///cl_f5.db\")\n",
    "df_cl5 = study_cl5.trials_dataframe(attrs=(\"params\", \"user_attrs\", \"value\", \"state\", \"datetime_start\", \"datetime_complete\"))\n",
    "df_cl5.head(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
