{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b024b07",
   "metadata": {},
   "source": [
    "1. Files and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41b72e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, glob, shutil, subprocess as sp\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import optuna\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c434ca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = Path(\"/home/lfuchs\")\n",
    "SCRIPTS = HOME / \"masterarbeit\" / \"scripts\"\n",
    "CONFIG  = HOME / \"masterarbeit\" / \"config\"\n",
    "\n",
    "REQUEST_TPL_DIR = SCRIPTS / \"request_templates\"\n",
    "REQUEST_TPL_NAME = \"backtest_template.request.j2\" \n",
    "REQUEST_OUT_DIR  = HOME / \"simulation-requests\"\n",
    "RESULTS_BASE     = HOME / \"simulation-results\"\n",
    "\n",
    "GREPPER = Path(\"/home/mgalas/git/FWTClientRepo/src/main/python/fwt/tearsheets/Grepper.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0db750",
   "metadata": {},
   "source": [
    "2. Experimental Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d51f2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = {\n",
    "    \"f1\": {\n",
    "        \"req_start\":  \"2001-01-02\",\n",
    "        \"req_end\":    \"2009-12-31\",\n",
    "        \"eval_start\": \"2005-01-03\",\n",
    "        \"eval_end\":   \"2009-12-31\",\n",
    "        \"crash\":      \"2007-07-19\",\n",
    "    },\n",
    "    \"f2\": {\n",
    "        \"req_start\":  \"2006-01-03\",\n",
    "        \"req_end\":    \"2014-12-31\",\n",
    "        \"eval_start\": \"2010-01-04\",\n",
    "        \"eval_end\":   \"2014-12-31\",\n",
    "        \"crash\":      \"2011-07-21\",\n",
    "    },\n",
    "    \"f3\": {\n",
    "        \"req_start\":  \"2011-01-03\",\n",
    "        \"req_end\":    \"2019-12-31\",\n",
    "        \"eval_start\": \"2015-01-02\",\n",
    "        \"eval_end\":   \"2019-12-31\",\n",
    "        \"crash\":      \"2018-09-20\",\n",
    "    },\n",
    "    \"f4\": {\n",
    "        \"req_start\":  \"2016-01-04\",\n",
    "        \"req_end\":    \"2024-12-31\",\n",
    "        \"eval_start\": \"2020-01-02\",\n",
    "        \"eval_end\":   \"2024-12-31\",\n",
    "        \"crash\":      \"2020-03-23\",\n",
    "    },\n",
    "     \"f5\": {\n",
    "        \"req_start\": \"2001-01-02\",\n",
    "        \"req_end\":   \"2015-12-31\",\n",
    "        \"eval1_start\": \"2005-01-03\",\n",
    "        \"eval1_end\":   \"2009-12-31\",\n",
    "        \"crash1\":      \"2007-07-19\",\n",
    "        \"eval2_start\": \"2010-01-04\",\n",
    "        \"eval2_end\":   \"2014-12-31\",\n",
    "        \"crash2\":      \"2011-07-21\",\n",
    "    },\n",
    "\n",
    "    \"f6\": {\n",
    "        \"req_start\": \"2001-01-02\",\n",
    "        \"req_end\":   \"2020-12-31\",\n",
    "        \"eval1_start\": \"2005-01-03\",\n",
    "        \"eval1_end\":   \"2009-12-31\",\n",
    "        \"crash1\":      \"2007-07-19\",\n",
    "        \"eval2_start\": \"2010-01-04\",\n",
    "        \"eval2_end\":   \"2014-12-31\",\n",
    "        \"crash2\":      \"2011-07-21\",\n",
    "        \"eval3_start\": \"2015-01-02\",\n",
    "        \"eval3_end\":   \"2019-12-31\",\n",
    "        \"crash3\":      \"2018-09-20\",\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3618ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility function which calculates utility based on performance metrics in ZQQ.csv ---\n",
    "# Note: has to be updated if the utility function definition changes\n",
    "def utility_from_zqq(\n",
    "    zqq_csv: str | Path,\n",
    "    fold_start: str, fold_end: str,\n",
    "    crash_start: str,\n",
    "    return_components: bool = False,\n",
    "    lambdas=(0.35, 0.20, 0.15, 0.30)\n",
    "):\n",
    "   \n",
    "    lam1, lam2, lam3, lam4 = lambdas\n",
    "\n",
    "    df = pd.read_csv(zqq_csv)\n",
    "    date_col = df.columns[1]\n",
    "    fund_col = df.columns[2]\n",
    "    bench_col = df.columns[3]\n",
    "    df = df.rename(columns={date_col: \"Date\", fund_col: \"Fund\", bench_col: \"Benchmark\"})\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df = df.set_index(\"Date\").sort_index()\n",
    "\n",
    "    # Restrict to fold period\n",
    "    fold_start = pd.to_datetime(fold_start)\n",
    "    fold_end   = pd.to_datetime(fold_end)\n",
    "    df_fold = df.loc[(df.index >= fold_start) & (df.index <= fold_end)]\n",
    "    if df_fold.empty:\n",
    "        return float(\"-inf\")\n",
    "\n",
    "    # First available day t\n",
    "    t = df_fold.index[0]\n",
    "\n",
    "    # Helper: find next available date ≤ target\n",
    "    def pick_leq(target):\n",
    "        target = pd.to_datetime(target)\n",
    "        ix = df_fold.index.searchsorted(target, side=\"right\") - 1\n",
    "        if ix < 0: ix = 0\n",
    "        return df_fold.index[ix]\n",
    "\n",
    "    # Calculate relative returns\n",
    "    def rel(t1, t2):\n",
    "        a = df_fold.loc[t2, \"Fund\"] / df_fold.loc[t1, \"Fund\"]\n",
    "        m = df_fold.loc[t2, \"Benchmark\"] / df_fold.loc[t1, \"Benchmark\"]\n",
    "        return float(a), float(m)\n",
    "\n",
    "    # f1: 5y annualised minus 3.5%\n",
    "    t1 = pick_leq(t + pd.DateOffset(years=5))\n",
    "    a1, m1 = rel(t, t1)\n",
    "    f1 = (a1 ** (1/5)) - (m1 ** (1/5)) - 0.035\n",
    "\n",
    "    # f2: 1y relative\n",
    "    t2 = pick_leq(t + pd.DateOffset(years=1))\n",
    "    a2, m2 = rel(t, t2)\n",
    "    f2 = a2 - m2\n",
    "\n",
    "    # f3: 3m relative annualised (^4) + 0.2\n",
    "    t3 = pick_leq(t + pd.DateOffset(months=3))\n",
    "    a3, m3 = rel(t, t3)\n",
    "    f3 = (a3 ** 4) - (m3 ** 4) + 0.2\n",
    "\n",
    "    # f4: Integral over 2 years starting at crash_start\n",
    "    crash_start = pick_leq(pd.to_datetime(crash_start))\n",
    "    crash_end   = pick_leq(crash_start + pd.DateOffset(years=2))\n",
    "    df_win = df_fold.loc[(df_fold.index >= crash_start) & (df_fold.index <= crash_end)].copy()\n",
    "    if df_win.empty:\n",
    "        f4 = 0.0\n",
    "    else:\n",
    "        # Normalisation at crash start\n",
    "        Ac = float(df_fold.loc[crash_start, \"Fund\"])\n",
    "        Sc = float(df_fold.loc[crash_start, \"Benchmark\"])\n",
    "        fund_norm  = df_win[\"Fund\"] / Ac\n",
    "        bench_norm = df_win[\"Benchmark\"] / Sc\n",
    "        df_win[\"diff\"] = np.sqrt(fund_norm) - np.sqrt(bench_norm)\n",
    "        days = (df_win.index - df_win.index[0]).days.values\n",
    "        f4 = np.trapz(df_win[\"diff\"].values, x=days) / 2.0\n",
    "\n",
    "    U = lam1*f1 + lam2*f2 + lam3*f3 + lam4*f4\n",
    "\n",
    "    if return_components:\n",
    "        return float(U), float(f1), float(f2), float(f3), float(f4)\n",
    "    return float(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a9876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6e15b0f",
   "metadata": {},
   "source": [
    "3. Lambda Sensivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42852843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_sensitivity_row(study_name, zqq_path, fold):\n",
    "    if study_name == \"cl_f1\":\n",
    "        U_base, f1, f2, f3, f4 = utility_from_zqq(\n",
    "            zqq_path,\n",
    "            fold[\"eval_start\"], fold[\"eval_end\"], fold[\"crash\"],\n",
    "            return_components=True\n",
    "        )\n",
    "    elif study_name == \"cl_f5\":\n",
    "        U1, a1, b1, c1, d1 = utility_from_zqq(\n",
    "            zqq_path,\n",
    "            fold[\"eval1_start\"], fold[\"eval1_end\"], fold[\"crash1\"],\n",
    "            return_components=True\n",
    "        )\n",
    "        U2, a2, b2, c2, d2 = utility_from_zqq(\n",
    "            zqq_path,\n",
    "            fold[\"eval2_start\"], fold[\"eval2_end\"], fold[\"crash2\"],\n",
    "            return_components=True\n",
    "        )\n",
    "        U_base = 0.5*(U1+U2)\n",
    "        f1, f2, f3, f4 = 0.5*(a1+a2), 0.5*(b1+b2), 0.5*(c1+c2), 0.5*(d1+d2)\n",
    "    else:\n",
    "        raise ValueError(\"Nur cl_f1 und cl_f5.\")\n",
    "    \n",
    "ZQQ_BEST = {\n",
    "    \"cl_f1\": \"/home/lfuchs/simulation-results/cl_f1_b6_t20/ZQQ_cl_f1_b6_t20.csv\",\n",
    "    \"cl_f5\": \"/home/lfuchs/simulation-results/cl_f5_b2_t11/ZQQ_cl_f5_b2_t11.csv\",\n",
    "}\n",
    "\n",
    "\n",
    "LAMBDA_BASE = (0.35, 0.20, 0.15, 0.30)\n",
    "DELTAS     = [0.05]\n",
    "\n",
    "MOVE_PAIRS = [\n",
    "    (0,1), (1,0),\n",
    "    (0,2), (2,0),\n",
    "    (0,3), (3,0),\n",
    "    (1,2), (2,1),\n",
    "    (1,3), (3,1),\n",
    "    (2,3), (3,2),\n",
    "]\n",
    "\n",
    "\n",
    "def _move_lambda(lams, i_from, i_to, delta):\n",
    "    l = list(lams)\n",
    "    if l[i_from] - delta < 0:\n",
    "        return None\n",
    "    l[i_from] -= delta\n",
    "    l[i_to]   += delta\n",
    "    return tuple(l)\n",
    "\n",
    "def _label(i_from, i_to, delta):\n",
    "    return f\"{i_from+1}→{i_to+1} ±{delta:.02f}\"\n",
    "\n",
    "# --- Eval-Funktionen, die f1 und f5 korrekt behandeln ---\n",
    "def _eval_cl_f1(zqq_path, lambdas):\n",
    "    f = FOLDS[\"f1\"]\n",
    "    return utility_from_zqq(\n",
    "        zqq_path,\n",
    "        f[\"eval_start\"], f[\"eval_end\"], f[\"crash\"],\n",
    "        lambdas=lambdas,\n",
    "        return_components=False,\n",
    "    )\n",
    "\n",
    "def _eval_cl_f5(zqq_path, lambdas):\n",
    "    f = FOLDS[\"f5\"]\n",
    "    u1 = utility_from_zqq(\n",
    "        zqq_path, f[\"eval1_start\"], f[\"eval1_end\"], f[\"crash1\"],\n",
    "        lambdas=lambdas, return_components=False\n",
    "    )\n",
    "    u2 = utility_from_zqq(\n",
    "        zqq_path, f[\"eval2_start\"], f[\"eval2_end\"], f[\"crash2\"],\n",
    "        lambdas=lambdas, return_components=False\n",
    "    )\n",
    "    return float((u1 + u2) / 2.0)\n",
    "\n",
    "def _eval_study(study, zqq_path, lambdas):\n",
    "    if study == \"cl_f1\":\n",
    "        return _eval_cl_f1(zqq_path, lambdas)\n",
    "    if study == \"cl_f5\":\n",
    "        return _eval_cl_f5(zqq_path, lambdas)\n",
    "    raise ValueError(\"Nur cl_f1 und cl_f5.\")\n",
    "\n",
    "def lambda_sensitivity_for(study, zqq_path):\n",
    "    # Basis-Utility\n",
    "    u_base = _eval_study(study, zqq_path, LAMBDA_BASE)\n",
    "\n",
    "    # Varianten (pairwise shifts)\n",
    "    rows = []\n",
    "    for (i_from, i_to) in MOVE_PAIRS:\n",
    "        for d in DELTAS:\n",
    "            lams = _move_lambda(LAMBDA_BASE, i_from, i_to, d)\n",
    "            if lams is None:\n",
    "                continue\n",
    "            u = _eval_study(study, zqq_path, lams)\n",
    "            rows.append({\n",
    "                \"study\": study,\n",
    "                \"variant\": _label(i_from, i_to, d),\n",
    "                \"lambda1\": lams[0], \"lambda2\": lams[1],\n",
    "                \"lambda3\": lams[2], \"lambda4\": lams[3],\n",
    "                \"U\": float(u),\n",
    "                \"ΔU\": float(u - u_base),  # Diff zur Basis\n",
    "            })\n",
    "\n",
    "    df_long = pd.DataFrame(rows).sort_values([\"study\",\"U\"], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Wide: eine Spalte pro Variante mit ΔU\n",
    "    df_wide = df_long.pivot(index=\"study\", columns=\"variant\", values=\"ΔU\").reset_index()\n",
    "\n",
    "    # Summary je Study\n",
    "    summaries = []\n",
    "    for s, g in df_long.groupby(\"study\"):\n",
    "        U_best  = g[\"U\"].max()\n",
    "        U_worst = g[\"U\"].min()\n",
    "        summaries.append({\n",
    "            \"study\": s,\n",
    "            \"U_base\": float(u_base) if s == study else float(_eval_study(s, ZQQ_BEST[s], LAMBDA_BASE)),\n",
    "            \"U_best\": float(U_best),\n",
    "            \"ΔU_best\": float(U_best - (u_base if s == study else _eval_study(s, ZQQ_BEST[s], LAMBDA_BASE))),\n",
    "            \"U_worst\": float(U_worst),\n",
    "            \"ΔU_worst\": float(U_worst - (u_base if s == study else _eval_study(s, ZQQ_BEST[s], LAMBDA_BASE))),\n",
    "            \"ΔU_avg\": float((g[\"ΔU\"]).mean()),\n",
    "            \"best_variant\": g.loc[g[\"U\"].idxmax(), \"variant\"],\n",
    "        })\n",
    "    df_summary = pd.DataFrame(summaries).sort_values(\"study\").reset_index(drop=True)\n",
    "    return df_long, df_wide, df_summary\n",
    "\n",
    "# --- Ausführen für cl_f1 und cl_f5 ---\n",
    "df_long_all   = []\n",
    "df_wide_list  = []\n",
    "df_summ_list  = []\n",
    "\n",
    "for study in [\"cl_f1\", \"cl_f5\"]:\n",
    "    zqq = ZQQ_BEST[study]\n",
    "    dlong, dwide, dsum = lambda_sensitivity_for(study, zqq)\n",
    "    df_long_all.append(dlong)\n",
    "    df_wide_list.append(dwide)\n",
    "    df_summ_list.append(dsum)\n",
    "\n",
    "df_long_all = pd.concat(df_long_all, ignore_index=True)\n",
    "df_wide_all = pd.concat(df_wide_list, ignore_index=True)\n",
    "df_summary  = pd.concat(df_summ_list, ignore_index=True)\n",
    "\n",
    "print(\"ΔU je Variante (long):\")\n",
    "display(df_long_all)\n",
    "\n",
    "print(\"\\nΔU je Variante (wide, eine Spalte pro Shift):\")\n",
    "display(df_wide_all)\n",
    "\n",
    "print(\"\\nKurz-Zusammenfassung je Study:\")\n",
    "display(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e339d",
   "metadata": {},
   "source": [
    "3. Delta Sensivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3399ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sectors():\n",
    "    space = load_search_space()           # nutzt eure bestehende Funktion\n",
    "    return list(space[\"sectors\"])\n",
    "\n",
    "def best_params_as_request(study_name: str,\n",
    "                           storage: str,\n",
    "                           req_start: str, req_end: str) -> dict:\n",
    "    \"\"\"\n",
    "    Lädt den Best-Trial aus der Optuna-DB und baut exakt das params-Dict,\n",
    "    das generate_request_from_dict erwartet (inkl. VE1..VE20, GE1..GE20).\n",
    "    \"\"\"\n",
    "    sectors = _sectors()\n",
    "    st = optuna.load_study(study_name=study_name, storage=storage)\n",
    "    p  = st.best_trial.params\n",
    "\n",
    "    params = {\n",
    "        \"STARTDATE\": req_start,\n",
    "        \"ENDDATE\":   req_end,\n",
    "        \"NPORT\": int(p[\"NPORT\"]),\n",
    "        \"NFREQ\": int(p[\"NFREQ\"]),\n",
    "        \"VE\": {}, \"GE\": {}\n",
    "    }\n",
    "    for i, sec in enumerate(sectors, start=1):\n",
    "        v = float(p.get(f\"VE::{sec}\", 0.0))\n",
    "        g = float(p.get(f\"GE::{sec}\", 0.0))\n",
    "        params[\"VE\"][sec] = v\n",
    "        params[\"GE\"][sec] = g\n",
    "        params[f\"VE{i}\"] = v\n",
    "        params[f\"GE{i}\"] = g\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7bb85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _renorm_to_sum(x: np.ndarray, target_sum: float,\n",
    "                   lo: float = 0.01, hi: float = 0.99) -> np.ndarray:\n",
    "    \"\"\"Clipping auf [lo, hi] und Summe iterativ auf target_sum skalieren.\"\"\"\n",
    "    x = np.clip(x, lo, hi)\n",
    "    for _ in range(3):\n",
    "        s = x.sum()\n",
    "        if s == 0:\n",
    "            break\n",
    "        x = np.clip(x * (target_sum / s), lo, hi)\n",
    "    return x\n",
    "\n",
    "def jitter_params(params: dict, sigma: float = 0.02, seed: int | None = None) -> dict:\n",
    "    \"\"\"\n",
    "    Fügt VE/GE sektorweise N(0, sigma) hinzu, clippt auf [0.01, 0.99] und\n",
    "    hält die jeweilige Gesamtsumme von VE bzw. GE konstant.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    sectors = _sectors()\n",
    "\n",
    "    pert = copy.deepcopy(params)\n",
    "\n",
    "    # VE\n",
    "    ve = np.array([float(pert[\"VE\"][s]) for s in sectors], dtype=float)\n",
    "    ve_sum = ve.sum()\n",
    "    ve = _renorm_to_sum(ve + rng.normal(0, sigma, size=ve.size), ve_sum)\n",
    "    for i, s in enumerate(sectors, start=1):\n",
    "        pert[\"VE\"][s] = float(ve[i-1]); pert[f\"VE{i}\"] = float(ve[i-1])\n",
    "\n",
    "    # GE\n",
    "    ge = np.array([float(pert[\"GE\"][s]) for s in sectors], dtype=float)\n",
    "    ge_sum = ge.sum()\n",
    "    ge = _renorm_to_sum(ge + rng.normal(0, sigma, size=ge.size), ge_sum)\n",
    "    for i, s in enumerate(sectors, start=1):\n",
    "        pert[\"GE\"][s] = float(ge[i-1]); pert[f\"GE{i}\"] = float(ge[i-1])\n",
    "\n",
    "    return pert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _wait_for_result_dir(label: str, wait_hours: float = 8, poll_secs: int = 60) -> Path | None:\n",
    "    result_dir = RESULTS_BASE / label\n",
    "    deadline = time.time() + wait_hours * 3600\n",
    "    while time.time() < deadline:\n",
    "        if result_dir.exists():\n",
    "            return result_dir\n",
    "        time.sleep(poll_secs)\n",
    "    return None\n",
    "\n",
    "def _score_zqq_for_fold(label: str, zqq_path: str, fold_key: str) -> float:\n",
    "    \"\"\"Utility passend zu f1 (ein Fenster) / f5 (zwei Fenster, Mittelwert).\"\"\"\n",
    "    f = FOLDS[fold_key]\n",
    "    if fold_key == \"f1\":\n",
    "        return float(utility_from_zqq(zqq_path, f[\"eval_start\"], f[\"eval_end\"], f[\"crash\"]))\n",
    "    elif fold_key == \"f5\":\n",
    "        u1 = utility_from_zqq(zqq_path, f[\"eval1_start\"], f[\"eval1_end\"], f[\"crash1\"])\n",
    "        u2 = utility_from_zqq(zqq_path, f[\"eval2_start\"], f[\"eval2_end\"], f[\"crash2\"])\n",
    "        return float((u1 + u2) / 2.0)\n",
    "    else:\n",
    "        raise ValueError(\"Nur f1 und f5 sind hier vorgesehen.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27708cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_local_robustness_CL(study_key: str,\n",
    "                            storage: str,\n",
    "                            n_samples: int = 5,\n",
    "                            sigma: float = 0.02,\n",
    "                            seed: int = 0,\n",
    "                            label_prefix: str = \"robust\",\n",
    "                            wait_hours: float = 8,\n",
    "                            poll_secs: int = 60) -> pd.DataFrame:\n",
    "   \n",
    "    assert study_key in (\"cl_f1\", \"cl_f5\")\n",
    "    fold_key = \"f1\" if study_key == \"cl_f1\" else \"f5\"\n",
    "    f = FOLDS[fold_key]\n",
    "\n",
    "    # 1) Best-Trial → Request-Params (unverändert)\n",
    "    base = best_params_as_request(study_name=study_key, storage=storage,\n",
    "                                  req_start=f[\"req_start\"], req_end=f[\"req_end\"])\n",
    "\n",
    "    rows = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # 2) Jitter\n",
    "        pert = jitter_params(base, sigma=sigma, seed=int(rng.integers(0, 2**31-1)))\n",
    "\n",
    "        # 3) Request schreiben & Job starten\n",
    "        label = f\"{label_prefix}_{study_key}_s{i}\"\n",
    "        req_file = f\"{label}.request\"\n",
    "        generate_request_from_dict(pert, req_file)\n",
    "\n",
    "        # 4) warten bis Result-Ordner da ist\n",
    "        result_dir = _wait_for_result_dir(label, wait_hours=wait_hours, poll_secs=poll_secs)\n",
    "        if result_dir is None:\n",
    "            rows.append({\"sample\": i, \"label\": label, \"U\": float(\"nan\"), \"status\": \"TIMEOUT\"})\n",
    "            continue\n",
    "\n",
    "        # 5) ZQQ erzeugen und Utility rechnen\n",
    "        zqq_path = make_zqq_csv(result_dir, label)\n",
    "        U = _score_zqq_for_fold(label, zqq_path, fold_key)\n",
    "\n",
    "        rows.append({\"sample\": i, \"label\": label, \"U\": float(U), \"status\": \"OK\", \"zqq\": zqq_path})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# cl_f1: 5 Jitter-Samples\n",
    "df_rb_f1 = run_local_robustness_CL(\n",
    "    study_key=\"cl_f1\",\n",
    "    storage=\"sqlite:///cl_f1.db\",\n",
    "    n_samples=5,\n",
    "    sigma=0.02,          # ±2%-Rauschen\n",
    "    seed=123,\n",
    "    label_prefix=\"robust\"\n",
    ")\n",
    "print(\"Robustness cl_f1:\")\n",
    "display(df_rb_f1)\n",
    "\n",
    "# cl_f5: 5 Jitter-Samples\n",
    "df_rb_f5 = run_local_robustness_CL(\n",
    "    study_key=\"cl_f5\",\n",
    "    storage=\"sqlite:///cl_f5.db\",\n",
    "    n_samples=5,\n",
    "    sigma=0.02, \n",
    "    seed=123,\n",
    "    label_prefix=\"robust\"\n",
    ")\n",
    "print(\"Robustness cl_f5:\")\n",
    "display(df_rb_f5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
